[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Navya Reddy Dontham",
    "section": "",
    "text": "An enthusiastic and self-motivated Software Engineer with an extensive experience of 2 years in designing, developing, and testing in backend and front-end development. Strong background in building Applications using various technologies. Capable of working in a team environment or working independently. I am continually seeking new challenges and a desire to expand knowledge and experience."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "As a dedicated and self-driven Software Engineer, I bring over two years of extensive experience in designing, developing, and testing SQL queries for applications. My proficiency extends to both backend and front-end development, showcasing strong skills in HTML5, CSS3, JavaScript, C#, SQL, ADO.NET, Angular, and ASP.NET (API & MVC). Throughout my career, I have demonstrated competence in analyzing requirements, providing solutions, and collaborating with cross-functional teams. I am well-versed in the full Software Development Lifecycle, object-oriented programming, and agile methodologies. My role as an SQL Developer Intern at Znalytics Business Solutions involved maintaining procedural codes, collaborating with diverse teams, and troubleshooting issues to deliver high-quality products within deadlines. In my recent position as a Software Engineer at Capgemini, I actively contributed to the development of a Laundry Management System. My responsibilities included backend development, implementing CRUD functionalities, and ensuring code quality through extensive testing using tools like Swagger. I possess a keen understanding of business and industry requirements, allowing me to identify and correct deviations from database development standards. I hold a Bachelor of Technology in Electronics and Communication Engineering from the Hyderabad Institute of Technology and Management (JNTUH), India. My core competencies encompass software development, general programming skills, software debugging, software documentation, software testing, and analytical problem-solving skills. Proficient in a range of technologies including JavaScript, C#, SQL, Angular, and ASP.NET, I am also adept at using tools such as Postman, Swagger, GitHub, Azure, Visual Studio, SQL Server Data Tools (SSDT), and SQL Server Management Studio (SSMS). During my internship, I gained hands-on experience in creating dynamic web page elements, implementing form error validations, and designing web pages using various technologies. I also contributed to projects involving Smart Home System design on FPGA in VLSI, Electronic Hardware design, and telemetry analysis. In summary, I am a versatile and committed professional with a proven track record in software engineering, backed by a solid educational foundation and a passion for staying abreast of new challenges in the ever-evolving field of technology."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Surya Vardhan",
    "section": "",
    "text": "Welcome to my corner of the internet! I’m Surya Vardhan, a passionate individual deeply engaged in the world of technology and data. With a curiosity-driven mindset, I explore the realms of data science, machine learning, and beyond. Join me on this journey of discovery and innovation!"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Navya Reddy Dontham",
    "section": "",
    "text": "An enthusiastic and self-motivated Software Engineer with an extensive experience of 2 years in designing, developing, and testing in backend and front-end development. Strong background in building Applications using various technologies. Capable of working in a team environment or working independently. I am continually seeking new challenges and a desire to expand knowledge and experience."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Data framing with R\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplotly\n\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nNavya reddy\n\n\n\n\n\n\n  \n\n\n\n\nExploratory data analysis and machine learning on Iris dataset\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nplotly\n\n\nplot\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nNavya reddy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/postwithplot/index.html",
    "href": "posts/postwithplot/index.html",
    "title": "Exploratory data analysis and machine learning on Iris dataset",
    "section": "",
    "text": "The script loads and explores the Iris dataset, visualizing features like sepal length and petal dimensions. It then implements a decision tree model, assesses its accuracy on a test set, and presents a confusion matrix. The script provides insights through EDA and demonstrates a basic machine learning approach for species classification in the Iris dataset.\n\n# Assuming IRSI_data is your dataset\n# You need to replace it with the actual name of your dataset\n\n# Load necessary libraries\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n# Load your IRSI data (replace \"your_dataset.csv\" with your actual dataset file)\nIRSI_data &lt;- read.csv(\"C:/Users/HP/Downloads/Iris.csv\")\n\n# Display the first few rows of the dataset\nhead(IRSI_data)\n\n  Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm     Species\n1  1           5.1          3.5           1.4          0.2 Iris-setosa\n2  2           4.9          3.0           1.4          0.2 Iris-setosa\n3  3           4.7          3.2           1.3          0.2 Iris-setosa\n4  4           4.6          3.1           1.5          0.2 Iris-setosa\n5  5           5.0          3.6           1.4          0.2 Iris-setosa\n6  6           5.4          3.9           1.7          0.4 Iris-setosa\n\n# Summary statistics\nsummary(IRSI_data)\n\n       Id         SepalLengthCm    SepalWidthCm   PetalLengthCm  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.054   Mean   :3.759  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  PetalWidthCm     Species         \n Min.   :0.100   Length:150        \n 1st Qu.:0.300   Class :character  \n Median :1.300   Mode  :character  \n Mean   :1.199                     \n 3rd Qu.:1.800                     \n Max.   :2.500                     \n\n# Distribution plot for Sepal.Length\nggplot(IRSI_data, aes(x = SepalLengthCm)) +\n  geom_histogram(binwidth = 0.1, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Sepal Length\",\n       x = \"Sepal Length\",\n       y = \"Frequency\")\n\n\n\n# Boxplot for Sepal.Length by Species\nggplot(IRSI_data, aes(x = Species, y = SepalWidthCm)) +\n  geom_boxplot(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Boxplot of Sepal Length by Species\",\n       x = \"Species\",\n       y = \"Sepal Length\")\n\n\n\n# Scatter plot for Petal.Length and Petal.Width\nggplot(IRSI_data, aes(x = PetalLengthCm, y = PetalWidthCm, color = Species)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Petal Length vs Petal Width\",\n       x = \"Petal Length\",\n       y = \"Petal Width\",\n       color = \"Species\")\n\n\n\n# Pair plot\npairs(IRSI_data[, c(\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\")], \n      main = \"Pair Plot of Iris Data\",\n      pch = 21, bg = c(\"red\", \"green3\", \"blue\")[unclass(IRSI_data$Species)])\n\n\n\n# Correlation matrix\ncor_matrix &lt;- cor(IRSI_data[, c(\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\")])\n\n# Violin plot\nggplot(IRSI_data, aes(x = Species, y = PetalLengthCm, fill = Species)) +\n  geom_violin(trim = FALSE) +\n  labs(title = \"Violin Plot of Petal Length by Species\",\n       x = \"Species\",\n       y = \"Petal Length\",\n       fill = \"Species\")\n\n\n\n# Density plot\nggplot(IRSI_data, aes(x = PetalWidthCm, fill = Species)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot of Petal Width by Species\",\n       x = \"Petal Width\",\n       fill = \"Species\")\n\n\n\n# Load necessary library for machine learning\nlibrary(rpart)\n\nWarning: package 'rpart' was built under R version 4.3.2\n\n# Split the data into training and testing sets\nset.seed(123)\nsample_index &lt;- sample(1:nrow(IRSI_data), 0.7 * nrow(IRSI_data))\ntrain_data &lt;- IRSI_data[sample_index, ]\ntest_data &lt;- IRSI_data[-sample_index, ]\n\n# Build a decision tree model\niris_model &lt;- rpart(Species ~ SepalLengthCm + SepalWidthCm + PetalLengthCm + PetalWidthCm, data = train_data, method = \"class\")\n\n# Visualize the decision tree\nplot(iris_model)\ntext(iris_model, cex = 0.8)\n\n\n\n# Make predictions on the test set\npredictions &lt;- predict(iris_model, newdata = test_data, type = \"class\")\n\n# Evaluate the model\nconf_matrix &lt;- table(predictions, test_data$Species)\naccuracy &lt;- sum(diag(conf_matrix)) / sum(conf_matrix)\nprint(paste(\"Accuracy:\", round(accuracy, 2)))\n\n[1] \"Accuracy: 0.98\"\n\n# If you want to see the confusion matrix\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(conf_matrix)\n\n                 \npredictions       Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              14               0              0\n  Iris-versicolor           0              18              1\n  Iris-virginica            0               0             12"
  },
  {
    "objectID": "projects.html#quarto-blog---data-visualizations---animation-and-interactivity",
    "href": "projects.html#quarto-blog---data-visualizations---animation-and-interactivity",
    "title": "Surya Vadhan",
    "section": "",
    "text": "Description 1\n\n\n\nImage 1"
  },
  {
    "objectID": "projects.html#shiny-flex-dashboard---sales-forecasting-and-anomaly-detection",
    "href": "projects.html#shiny-flex-dashboard---sales-forecasting-and-anomaly-detection",
    "title": "Surya Vadhan",
    "section": "Shiny Flex Dashboard - Sales forecasting and anomaly detection",
    "text": "Shiny Flex Dashboard - Sales forecasting and anomaly detection\n\n\nImage 2\n\n\n\nDescription 2"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/new/index.html",
    "href": "posts/new/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/postwithplot/index.html#how-to-read-the-data",
    "href": "posts/postwithplot/index.html#how-to-read-the-data",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "The first row in NFLX data set: On 09‐Aug-2002 (ANNDATS), analyst 6749 (ANALYS) at Estimator 1872 (ESTIMATOR) predicts that the EPS (MEASURE) for NETFLIX INC. (CNAME) with a ticker of NFLX (TICKER) with forecast period ending 30‐Sep-2002 (FPEDATS) is -$0.0086 (VALUE). This estimates was entered into the database on 12‐Aug-2002 (ACTDATS). On 17-Oct-2002 (ANNDATS_ACT), NETFLIX INC. announced an actual EPS of $7e-04 ($0.0007) (ACTUAL) for this quarter (FPI=6).\n\n\nCode\nhead(NFLX,n=1)\n\n\n  TICKER        CNAME  ACTDATS ESTIMATOR ANALYS FPI MEASURE   VALUE  FPEDATS\n1   NFLX NETFLIX INC. 20020812      1872   6749   6     EPS -0.0086 20020930\n   REVDATS  REVTIMS  ANNDATS  ANNTIMS ACTUAL ANNDATS_ACT ANNTIMS_ACT\n1 20021018 17:02:56 20020809 14:00:00 -7e-04    20021017    17:04:00"
  },
  {
    "objectID": "posts/postwithplot/index.html#your-turn",
    "href": "posts/postwithplot/index.html#your-turn",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Task 1A: Calculate Missingness\n\n\n\nCheck to see the missing values in NFLX dataset and calculate the percent missing for each variable in NFLX and list your findings in R object called NFLX_missingness. NFLX_missingness is a dataframe with two columns: The first column, Variable, stores the variable names and the second column, Missingness shows the percent missing in percentage points with two decimal points."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-1a",
    "href": "posts/postwithplot/index.html#your-code-for-task-1a",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\nNFLX &lt;- read.csv(\"C:/Users/HP/Downloads/NFLX.csv\", header=TRUE)\n\nNFLX_missingness &lt;- NFLX %&gt;%\n  summarise_all(~mean(is.na(.)) * 100)\n\nNFLX_missingness_table &lt;- NFLX_missingness %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\nprint(NFLX_missingness_table)\n\n\n&lt;table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\"&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:right;\"&gt; TICKER &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; CNAME &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ACTDATS &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ESTIMATOR &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ANALYS &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; FPI &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; MEASURE &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; VALUE &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; FPEDATS &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; REVDATS &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; REVTIMS &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ANNDATS &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ANNTIMS &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ACTUAL &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ANNDATS_ACT &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; ANNTIMS_ACT &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 4.115304 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 4.115304 &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 0 &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n\nCode\ndata_long &lt;- gather(NFLX_missingness, key = \"Variable\", value = \"MissingPercentage\")\n\ncolor_palette &lt;- c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#FF7F00\", \"#F781BF\", \"#A65628\",\n                   \"#984EA3\", \"#999999\", \"#66C2A5\", \"#FC8D62\", \"#8EBA42\", \"#FFD92F\",\n                   \"#E7298A\", \"#1B9E77\", \"#D95F02\", \"#7570B3\")\n\nmissingness_plot &lt;- ggplot(data_long, aes(x = reorder(Variable, -MissingPercentage), y = MissingPercentage, fill = Variable)) +\n  geom_bar(stat = \"identity\", fill = color_palette) +\n  labs(title = \"Missing Values Percentage by Variable\", x = \"Variable\", y = \"Missing Percentage\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nprint(missingness_plot)\n\n\n\n\n\n\n\n\n\n\n\nTask 1B: Data Manipulation\n\n\n\nConduct the following data manipulations on NFLX:\n\nDrop rows from the data set when a variable has a missing value\nDrop rows from the data set the quarterly forecasts (drop FPI=6)\nDeclare TICKER, CNAME, ESTIMATOR , ANALYS, FPI , and MEASURE variables as factor\nDeclare ACTDATS, FPEDATS , ANNDATS, REVDATS, ANNDATS_ACT as time variable.\nDrop ANNTIMS_ACT, ANNTIMS , and REVTIMS\nCreate a new column named YEAR that captures the year in FPEDATS\nName your reduced dataset as NFLX1\nPrint out data structure and the summary of NFLX1"
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-1b",
    "href": "posts/postwithplot/index.html#your-code-for-task-1b",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Copy NFLX to NFLX1 without assigning data types\nNFLX1 &lt;- NFLX\n\n# Drop rows from the data set when a variable has a missing value\nNFLX1 &lt;- NFLX1 %&gt;% na.omit()\n\n# Drop rows from the data set where FPI=6\nNFLX1 &lt;- NFLX1 %&gt;% filter(FPI != 6)\n\n# Drop ANNTIMS_ACT, ANNTIMS, and REVTIMS\nNFLX1 &lt;- NFLX1 %&gt;% select(-ANNTIMS_ACT, -ANNTIMS, -REVTIMS)\n\n# Create a new column named YEAR that is an exact copy of the data in FPEDATS\nNFLX1 &lt;- NFLX1 %&gt;% mutate(YEAR = FPEDATS)\n\n# Print out data structure and the summary of NFLX1\nstr(NFLX1)\n\n\n'data.frame':   2603 obs. of  14 variables:\n $ TICKER     : chr  \"NFLX\" \"NFLX\" \"NFLX\" \"NFLX\" ...\n $ CNAME      : chr  \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" \"NETFLIX INC.\" ...\n $ ACTDATS    : int  20020805 20021202 20021202 20021202 20021205 20030106 20030115 20030116 20030121 20030314 ...\n $ ESTIMATOR  : int  183 2178 1872 220 2178 1872 2227 220 1872 481 ...\n $ ANALYS     : int  79868 80485 6749 57596 80485 6749 82629 57596 6749 81599 ...\n $ FPI        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ MEASURE    : chr  \"EPS\" \"EPS\" \"EPS\" \"EPS\" ...\n $ VALUE      : num  -0.025 -0.0321 -0.0207 -0.0179 -0.0286 -0.0136 -0.0164 -0.0071 0.0107 0.0129 ...\n $ FPEDATS    : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n $ REVDATS    : int  20021129 20021202 20021202 20021206 20021205 20030114 20030115 20030417 20030402 20030409 ...\n $ ANNDATS    : int  20020805 20021202 20021202 20021202 20021204 20030102 20030115 20030116 20030116 20030314 ...\n $ ACTUAL     : num  -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 -0.005 0.0393 0.0393 0.0393 ...\n $ ANNDATS_ACT: int  20030115 20030115 20030115 20030115 20030115 20030115 20030115 20040121 20040121 20040121 ...\n $ YEAR       : int  20021231 20021231 20021231 20021231 20021231 20021231 20021231 20031231 20031231 20031231 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:217] 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 ...\n  ..- attr(*, \"names\")= chr [1:217] \"5057\" \"5058\" \"5059\" \"5060\" ...\n\n\nCode\nsummary(NFLX1)\n\n\n    TICKER             CNAME              ACTDATS           ESTIMATOR   \n Length:2603        Length:2603        Min.   :20020805   Min.   :  11  \n Class :character   Class :character   1st Qu.:20101021   1st Qu.: 192  \n Mode  :character   Mode  :character   Median :20141009   Median : 899  \n                                       Mean   :20136831   Mean   :1376  \n                                       3rd Qu.:20180122   3rd Qu.:2502  \n                                       Max.   :20210119   Max.   :4439  \n     ANALYS            FPI      MEASURE              VALUE       \n Min.   :  1047   Min.   :1   Length:2603        Min.   :-0.150  \n 1st Qu.: 71755   1st Qu.:1   Class :character   1st Qu.: 0.190  \n Median : 82010   Median :1   Mode  :character   Median : 0.430  \n Mean   : 89534   Mean   :1                      Mean   : 1.339  \n 3rd Qu.:114459   3rd Qu.:1                      3rd Qu.: 2.015  \n Max.   :194536   Max.   :1                      Max.   : 7.670  \n    FPEDATS            REVDATS            ANNDATS             ACTUAL      \n Min.   :20021231   Min.   :20021129   Min.   :20020805   Min.   :-0.005  \n 1st Qu.:20101231   1st Qu.:20110120   1st Qu.:20101021   1st Qu.: 0.250  \n Median :20141231   Median :20141013   Median :20141009   Median : 0.430  \n Mean   :20137082   Mean   :20137740   Mean   :20136830   Mean   : 1.384  \n 3rd Qu.:20181231   3rd Qu.:20180122   3rd Qu.:20180122   3rd Qu.: 2.680  \n Max.   :20201231   Max.   :20210119   Max.   :20210119   Max.   : 6.080  \n  ANNDATS_ACT            YEAR         \n Min.   :20030115   Min.   :20021231  \n 1st Qu.:20110126   1st Qu.:20101231  \n Median :20150120   Median :20141231  \n Mean   :20145973   Mean   :20137082  \n 3rd Qu.:20190117   3rd Qu.:20181231  \n Max.   :20210119   Max.   :20201231  \n\n\n\n\n\n\n\n\nTask 2: Calculate Number of Analysts and Brokerage Houses\n\n\n\n\nCalculate the total number of unique analysts in NFLX1 dataset that provide forecasts each year and name your R object as NumberAnalyst\nCalculate the total number of unique brokerage houses (ESTIMATOR) in NFLX1 dataset that provide forecasts each year and name your R object as NumberBrokerage\nNeed Written Response in this callout: In which year(s) we have the highest number of unique analysts providing forecasts for NFLX ticker? In which year(s), we have the highest number of unique brokerage houses providing forecasts for the NFLX ticker.\n\n\nIn 2020 , Netflix received forecasts from a distinct set of analysts and brokerage firms. on the same year the number of unique brokerage firm was 44 on the same period the number of unique analyst was 46."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-2",
    "href": "posts/postwithplot/index.html#your-code-for-task-2",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Calculate the number of unique analysts by year\nNumberAnalyst &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(NumAnalysts = n_distinct(ANALYS)) %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE)  \n\n# Calculate the number of unique brokerages by year\nNumberBrokerage &lt;- NFLX1 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(NumBrokerage = n_distinct(ESTIMATOR)) %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\") %&gt;%\n  row_spec(0, color = \"red\", bold = TRUE)  \n\n# Print the results\nNumberAnalyst\n\n\n\n\n\nYEAR\nNumAnalysts\n\n\n\n\n20021231\n5\n\n\n20031231\n9\n\n\n20041231\n19\n\n\n20051231\n17\n\n\n20061231\n20\n\n\n20071231\n20\n\n\n20081231\n20\n\n\n20091231\n33\n\n\n20101231\n37\n\n\n20111231\n40\n\n\n20121231\n38\n\n\n20131231\n42\n\n\n20141231\n45\n\n\n20151231\n47\n\n\n20161231\n46\n\n\n20171231\n48\n\n\n20181231\n56\n\n\n20191231\n46\n\n\n20201231\n49\n\n\n\n\n\n\n\nCode\nNumberBrokerage\n\n\n\n\n\nYEAR\nNumBrokerage\n\n\n\n\n20021231\n5\n\n\n20031231\n8\n\n\n20041231\n18\n\n\n20051231\n17\n\n\n20061231\n19\n\n\n20071231\n18\n\n\n20081231\n21\n\n\n20091231\n32\n\n\n20101231\n38\n\n\n20111231\n35\n\n\n20121231\n36\n\n\n20131231\n43\n\n\n20141231\n40\n\n\n20151231\n46\n\n\n20161231\n45\n\n\n20171231\n49\n\n\n20181231\n54\n\n\n20191231\n43\n\n\n20201231\n44\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 3: Get the most recent forecast in each year\n\n\n\n\nIt is quite possible that an analyst makes multiple forecasts throughout the year for the same fiscal period. Remove observations from NFLX1 if an analyst has multiple predictions for the same year and keep the last one (the most recent forecast for each year). Name your new dataset as NFLX2. This step is crucial for successful execution of the following tasks. Print the dimension of NFLX2.\nCheck your work: If your NFLX2 dataset has 641 rows and 14 columns, then you are on the right track. If not, please seek help!"
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-3",
    "href": "posts/postwithplot/index.html#your-code-for-task-3",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\nNFLX2 &lt;- NFLX1 %&gt;%\n  group_by(ANALYS, YEAR) %&gt;%\n  filter(REVDATS == max(REVDATS)) %&gt;%\n  ungroup()\n\ndim_output &lt;- dim(NFLX2)\n\ndim_table &lt;- as.data.frame(dim_output) %&gt;%\n  kable(format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\") %&gt;%\n  row_spec(0, bold = TRUE, color = \"blue\") %&gt;%\n  column_spec(1, bold = TRUE, color = \"green\")\n\n# Print the styled dimension table\nprint(dim_table)\n\n\n&lt;table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\"&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:right;font-weight: bold;color: blue !important;\"&gt; dim_output &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:right;font-weight: bold;color: green !important;\"&gt; 641 &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:right;font-weight: bold;color: green !important;\"&gt; 14 &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n\n\n\n\n\n\n\n\n\nTask 4: Calculate past accuracy\n\n\n\n\nCreate a copy of NFLX2 and call it NFLX3\nFor every year within the dataset NFLX3, compute the forecasting performance of each analyst for the current year and store the results in a new column labeled accuracy. In the calculation of forecast performance, you can use the VALUE-ACTUAL as the forecast accuracy measure.\nFor each year in the NFLX3 dataset, compute the forecasting performance of each analyst from the previous year and store the results in a new column called past_accuracy\nAs an example, consider the year 2006, where analyst 1047, employed at brokerage house 464, provided an estimated end-of-period EPS of 0.0929 (VALUE). However, the actual EPS for that year turned out to be 0.1014 (ACTUAL), resulting in a forecasting error of -0.0085. Consequently, in the subsequent year, 2007, the past_accuracy metric for analyst 1047 would reflect this error by taking the value of -0.0085 (VALUE-ACTUAL).\nThis action will create some missing values and this is perfectly fine.\nIf your code produces 144 NAs, then you are on the right track.\nNote that we are creating copies of the original dataset at each step to facilitate error detection in case any mistakes occur during the process."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-4",
    "href": "posts/postwithplot/index.html#your-code-for-task-4",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\nNFLX3 &lt;- NFLX2\n\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ANALYS) %&gt;%\n  mutate(accuracy = VALUE - ACTUAL) %&gt;%\n  group_by(ANALYS) %&gt;%\n  arrange(YEAR) %&gt;%\n  mutate(past_accuracy = lag(accuracy))\n\nsum(is.na(NFLX3$past_accuracy))\n\n\n[1] 144\n\n\nCode\nsum(is.na(NFLX3$past_accuracy)) %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\n\nx\n\n\n\n\n144\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 5: Forecast Horizon\n\n\n\n\nThe longer the forecast horizon, the higher the uncertainty associated with EPS forecasts. To control for this fact, create a new column in NFLX3 called horizon that captures the forecast horizon (ANNDATS_ACT- ANNDATS) for each analyst.\nWe anticipate observing a negative correlation between accuracy and horizon. Typically, as the forecast horizon increases, the accuracy tends to decrease, and vice versa. However, in our dataset, there is an exception where we find a positive correlation between accuracy and horizon for one specific year. Write an R code to identify and determine which year exhibits this positive correlation.\nNeed Written Response in this callout: Enter the year in here.\n\nThe year was 2018 on the month of December 31 to be precise with a correlation value of 0.243."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-5",
    "href": "posts/postwithplot/index.html#your-code-for-task-5",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Your data manipulation code\nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  arrange(YEAR) %&gt;%\n  mutate(past_accuracy = lag(accuracy, default = NA))  # Correct lag function usage\n\n# Calculate the forecast horizon for each analyst\nNFLX3 &lt;- NFLX3 %&gt;%\n  mutate(horizon = as.numeric(difftime(ANNDATS_ACT, ANNDATS, units = \"days\")))\n\n# Calculate the correlation between accuracy and horizon for each year\ncorrelation_by_year &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(correlation = cor(accuracy, horizon, use = \"complete.obs\"))\n\n# Find the year with a positive correlation\npositive_corr_year &lt;- correlation_by_year %&gt;%\n  filter(correlation &gt; 0)\n\n# Print the year with a positive correlation using kable for formatting\nkable(positive_corr_year, format = \"html\", caption = \"Years with Positive Correlation\")\n\n\n\nYears with Positive Correlation\n\n\nYEAR\ncorrelation\n\n\n\n\n20111231\n0.2266475\n\n\n20121231\n0.0653612\n\n\n20131231\n0.0213975\n\n\n20151231\n0.0558202\n\n\n20181231\n0.2430011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6: Experience\n\n\n\n\nWe assume that if an analyst is monitoring a company for a long period of time, he/she is expected to make more informed predictions. Create a new column in NFLX3 called experience that counts the cumulative number of years the analyst monitor (have predictions) the company. Print the summary of experience column.\nHint: Try to use cumsum() function in R.\nNeed Written Response in this callout: Which analyst (s) has the highest number of experience in NFLX3 dataset and for how long do they monitor the NFLX ticker?\n\nThere were two analysts of unique identifier 72088 and 77748 with the highest experience who observed the NTFLX ticker for 17 years each as observed in the graph."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-6",
    "href": "posts/postwithplot/index.html#your-code-for-task-6",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Enter your code for Task 6 below \nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  mutate(experience = cumsum(!duplicated(YEAR)))\n\nmax_experience &lt;- NFLX3 %&gt;%\n  group_by(ANALYS) %&gt;%\n  summarise(experience = max(experience)) %&gt;%\n  filter(experience == max(experience)) %&gt;%\n  arrange(desc(experience))\n\nexperience_summary &lt;- NFLX3 %&gt;%\n  summarise(\n    Min = min(experience),\n    Max = max(experience),\n    Mean = mean(experience),\n    Median = median(experience),\n    SD = sd(experience)\n  )\n\nexperience_summary %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\n\nANALYS\nMin\nMax\nMean\nMedian\nSD\n\n\n\n\n1047\n1\n7\n4.000000\n4.0\n2.1602469\n\n\n1324\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n1717\n1\n7\n4.000000\n4.0\n2.1602469\n\n\n5963\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n6749\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n9584\n1\n8\n4.500000\n4.5\n2.4494897\n\n\n10214\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n14162\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n19083\n1\n1\n1.000000\n1.0\nNA\n\n\n19787\n1\n10\n5.500000\n5.5\n3.0276504\n\n\n22098\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n22953\n1\n15\n8.000000\n8.0\n4.4721360\n\n\n32416\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n46176\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n46442\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n47195\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n48155\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n48634\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n49454\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n51867\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n53012\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n54310\n1\n8\n4.500000\n4.5\n2.4494897\n\n\n54440\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n54636\n1\n7\n4.000000\n4.0\n2.1602469\n\n\n56585\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n57008\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n57596\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n58063\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n58066\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n58823\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n70566\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n71040\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n71466\n1\n11\n6.000000\n6.0\n3.3166248\n\n\n71470\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n71755\n1\n11\n6.000000\n6.0\n3.3166248\n\n\n72001\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n72088\n1\n17\n9.000000\n9.0\n5.0497525\n\n\n72772\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n73430\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n74427\n1\n1\n1.000000\n1.0\nNA\n\n\n77748\n1\n17\n9.000000\n9.0\n5.0497525\n\n\n78488\n1\n13\n7.000000\n7.0\n3.8944405\n\n\n78750\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n79868\n1\n1\n1.000000\n1.0\nNA\n\n\n80183\n1\n12\n6.500000\n6.5\n3.6055513\n\n\n80234\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n80485\n1\n1\n1.000000\n1.0\nNA\n\n\n80523\n1\n13\n7.000000\n7.0\n3.8944405\n\n\n81118\n1\n7\n3.666667\n4.0\n2.1213203\n\n\n81208\n1\n1\n1.000000\n1.0\nNA\n\n\n81599\n1\n8\n4.500000\n4.5\n2.4494897\n\n\n82010\n1\n10\n5.500000\n5.5\n3.0276504\n\n\n82079\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n82492\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n82629\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n82656\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n82890\n1\n5\n2.833333\n2.5\n1.4719601\n\n\n83539\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n85902\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n87229\n1\n8\n4.500000\n4.5\n2.4494897\n\n\n89663\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n91571\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n91718\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n104524\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n105361\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n105367\n1\n10\n5.500000\n5.5\n3.0276504\n\n\n105752\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n106603\n1\n9\n5.000000\n5.0\n2.7386128\n\n\n106993\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n107095\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n107179\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n107279\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n107983\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n108897\n1\n1\n1.000000\n1.0\nNA\n\n\n109838\n1\n1\n1.000000\n1.0\nNA\n\n\n110140\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n110206\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n110711\n1\n12\n6.500000\n6.5\n3.6055513\n\n\n111516\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n111523\n1\n1\n1.000000\n1.0\nNA\n\n\n113333\n1\n13\n7.000000\n7.0\n3.8944405\n\n\n114029\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n114392\n1\n7\n4.000000\n4.0\n2.1602469\n\n\n114459\n1\n14\n7.500000\n7.5\n4.1833001\n\n\n114980\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n115360\n1\n1\n1.000000\n1.0\nNA\n\n\n117990\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n118593\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n118746\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n119202\n1\n1\n1.000000\n1.0\nNA\n\n\n119704\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n120953\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n120988\n1\n12\n6.500000\n6.5\n3.6055513\n\n\n121968\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n123215\n1\n1\n1.000000\n1.0\nNA\n\n\n125613\n1\n1\n1.000000\n1.0\nNA\n\n\n125796\n1\n9\n5.000000\n5.0\n2.7386128\n\n\n125888\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n126149\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n126163\n1\n1\n1.000000\n1.0\nNA\n\n\n128228\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n128238\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n128394\n1\n1\n1.000000\n1.0\nNA\n\n\n129853\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n130849\n1\n1\n1.000000\n1.0\nNA\n\n\n131744\n1\n1\n1.000000\n1.0\nNA\n\n\n131848\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n133463\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n135251\n1\n1\n1.000000\n1.0\nNA\n\n\n135396\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n137026\n1\n9\n5.000000\n5.0\n2.7386128\n\n\n137103\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n137702\n1\n1\n1.000000\n1.0\nNA\n\n\n141291\n1\n1\n1.000000\n1.0\nNA\n\n\n142116\n1\n10\n5.500000\n5.5\n3.0276504\n\n\n147131\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n147369\n1\n5\n3.000000\n3.0\n1.5811388\n\n\n148735\n1\n1\n1.000000\n1.0\nNA\n\n\n148843\n1\n1\n1.000000\n1.0\nNA\n\n\n149065\n1\n6\n3.500000\n3.5\n1.8708287\n\n\n149262\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n149299\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n151995\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n152060\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n152603\n1\n1\n1.000000\n1.0\nNA\n\n\n160058\n1\n7\n4.000000\n4.0\n2.1602469\n\n\n164392\n1\n2\n1.333333\n1.0\n0.5773503\n\n\n164735\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n164750\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n171669\n1\n1\n1.000000\n1.0\nNA\n\n\n180339\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n182826\n1\n4\n2.500000\n2.5\n1.2909944\n\n\n183197\n1\n1\n1.000000\n1.0\nNA\n\n\n183261\n1\n3\n2.000000\n2.0\n1.0000000\n\n\n183736\n1\n1\n1.000000\n1.0\nNA\n\n\n184258\n1\n1\n1.000000\n1.0\nNA\n\n\n190823\n1\n1\n1.000000\n1.0\nNA\n\n\n191211\n1\n1\n1.000000\n1.0\nNA\n\n\n192063\n1\n1\n1.000000\n1.0\nNA\n\n\n192537\n1\n2\n1.500000\n1.5\n0.7071068\n\n\n192545\n1\n1\n1.000000\n1.0\nNA\n\n\n192551\n1\n1\n1.000000\n1.0\nNA\n\n\n193251\n1\n1\n1.000000\n1.0\nNA\n\n\n194536\n1\n1\n1.000000\n1.0\nNA\n\n\n\n\n\n\n\nCode\nmax_experience %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\") %&gt;%\n  row_spec(which(1:nrow(max_experience) %% 2 == 0), background = \"#F2F2F2\") %&gt;%\n  row_spec(which(1:nrow(max_experience) %% 2 != 0), background = \"#D9EAD3\")\n\n\n\n\n\nANALYS\nexperience\n\n\n\n\n72088\n17\n\n\n77748\n17\n\n\n\n\n\n\n\nCode\nggplot(max_experience, aes(x = reorder(ANALYS, -experience), y = experience, fill = experience)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Analyst(s) with Maximum Experience\",\n       x = \"Analyst\",\n       y = \"Experience\") +\n  scale_fill_viridis_c(option = \"magma\", direction = -1) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  guides(fill = guide_colorbar(title = \"Experience\")) \n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 7: Size\n\n\n\n\nIf a brokerage house has multiple analysts providing predictions for the same company, it may indicate a greater allocation of resources for company analysis. To capture this, create a new column in the NFLX3 dataset called size that calculates the total count of unique analysts employed per year by each brokerage house (ESTIMATOR)\nNeed Written Response in this callout: Print the frequencies for size variable. What does this frequency table reveal about the distribution of the number of analysts hired by brokerage houses in this dataset?\n\nThe graph shows a clear trend: as the number of analysts rises, the hiring frequency decreases significantly. A clear indication of hiring trend, in which the brokerage firm higher one analyst per season."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-7",
    "href": "posts/postwithplot/index.html#your-code-for-task-7",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\n# Enter your code for Task 7 below \nNFLX3 &lt;- NFLX3 %&gt;%\n  group_by(YEAR, ESTIMATOR) %&gt;%\n  mutate(size = n_distinct(ANALYS))\n\nsize_freq &lt;- table(NFLX3$size)\n\nsize_table &lt;- as.data.frame(size_freq)\ncolnames(size_table) &lt;- c(\"Number of Analysts\", \"Frequency\")\n\nsize_table &lt;- size_table[order(-size_table$Frequency), ]\n\nsize_table %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\n\nNumber of Analysts\nFrequency\n\n\n\n\n1\n560\n\n\n2\n72\n\n\n3\n9\n\n\n\n\n\n\n\nCode\nsummary_stats &lt;- summary(NFLX3$size)\n\nsummary_data &lt;- data.frame(\n  Statistic = names(summary_stats),\n  Value = as.character(summary_stats)\n)\n\nsummary_data %&gt;%\n  kable() %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nMin.\n1\n\n\n1st Qu.\n1\n\n\nMedian\n1\n\n\nMean\n1.14040561622465\n\n\n3rd Qu.\n1\n\n\nMax.\n3\n\n\n\n\n\n\n\nCode\nggplot(size_table, aes(x = factor(`Number of Analysts`), y = Frequency, fill = `Number of Analysts`)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"Number of Unique Analysts per Year and ESTIMATOR\") +\n  xlab(\"Number of Analysts\") +\n  ylab(\"Frequency\") +\n  scale_fill_manual(\n    values = c(\"#1f78b4\", \"#33a02c\", \"#e31a1c\", \"#ff7f00\", \"#6a3d9a\")  # Example colors, feel free to change\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 8: Prediction 1\n\n\n\n\nIn the year 2020, NETFLIX reported an actual earnings per share (EPS) of $6.08. To predict this EPS value based on historical data, we will employ a linear regression model using the dataset NFLX3 up until the year 2019. In this model, the target variable will be ACTUAL and the predictor variables will include VALUE and past_accuracy. C.all your model as model1.\nNeed Written Response in this callout: Using the linear regression model ‘model1,’ which has been trained on historical data up to the year 2019, what is the forecasted EPS (Earnings Per Share) for the year 2020? Please provide a brief explanation of the method you employed to make this prediction. If you encountered any challenges or were unable to make the calculation, briefly describe the specific issues you encountered.\n\n\n\n\n\n\n\nWhen utilizing the linear regression model ‘model1’, trained on historical data up until 2019, to forecast EPS for 2020, a rigorous evaluation of the model’s fit is conducted using the R-squared value. Should this metric surpass the 0.5 threshold, indicating a robust fit, the forecasting process commences. Central to this process is the calculation of the mean of the ‘past_accuracy’ variable. In instances where the R-squared value meets the prescribed criterion, a new dataset is meticulously curated, ensuring the incorporation of pertinent independent variables. Subsequently, EPS estimates for the future period are meticulously derived through the adept utilization of the ‘predict’ function.However, in scenarios where the R-squared value fails to meet the specified threshold, a cautionary warning emerges, signaling potential inaccuracies in the subsequent EPS predictions. Addressing specific nuances in data configuration or refining the model’s training approach becomes imperative, safeguarding the accuracy and reliability of future EPS projections."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-8",
    "href": "posts/postwithplot/index.html#your-code-for-task-8",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\nmean_past_accuracy &lt;- mean(NFLX3$past_accuracy, na.rm = TRUE)\n\nmodel1 &lt;- lm(ACTUAL ~ VALUE + past_accuracy, data = NFLX3)\n\n# Get the R-squared value of the model\nr_squared &lt;- summary(model1)$r.squared\n\n# If the R-squared value is high, then we can use the model to generate a forecast\nif (r_squared &gt; 0.5) {\n  new_data_future &lt;- data.frame(\n    VALUE = 6.08, # Replace this with the actual value of VALUE for the future period\n    past_accuracy = mean_past_accuracy\n  )\n\n  predicted_eps_future &lt;- predict(model1, newdata = new_data_future)\n  cat(\"Forecasted EPS for future period: $\", round(predicted_eps_future, 2))\n\n} else {\n\n  cat(\"The R-squared value is low, so the model may not be able to accurately predict future values of the dependent variable.\")\n\n}\n\n\nForecasted EPS for future period: $ 6.3\n\n\nCode\nkable(round(mean_past_accuracy, 2), col.names = \"Mean past_accuracy\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\")\n\n\n\n\n\nMean past_accuracy\n\n\n\n\n-0.09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 9: Prediction 2\n\n\n\n\nAs an alternative approach, instead of modeling the ‘ACTUAL’ value, we can obtain the mean and median forecasts for the year 2020 as our best estimates of the EPS value for that year.\nNeed Written Response in this callout: Please calculate these forecasts and then compare them with the results from the previous task. Finally, provide your insights and comments based on your findings.\n\nIn this alternative strategy, we have opted to derive our EPS forecasts for 2020 based on two fundamental metrics: the mean and median projections. These straightforward yet essential statistical measures offer valuable insights. The mean forecast, standing at approximately $1.24, reflects a balanced average of our expectations for 2020 EPS. In contrast, the median forecast, notably lower at around $0.41, underscores a central tendency amidst the data distribution. When juxtaposed with the model-driven approach explored earlier, it becomes apparent that the model’s complexity might yield a more nuanced and potentially accurate prediction. However, the simplicity of mean and median forecasts should not be underestimated. While lacking the sophistication of a well-fitted model, they provide accessible, high-level insights. The selection between these methodologies should hinge upon the data’s quality and the specific context of the analysis, emphasizing the need for a judicious approach in forecasting decisions."
  },
  {
    "objectID": "posts/postwithplot/index.html#your-code-for-task-9",
    "href": "posts/postwithplot/index.html#your-code-for-task-9",
    "title": "Mini Group Project 1",
    "section": "",
    "text": "Code\nmean_forecast &lt;- mean(NFLX3$VALUE, na.rm = TRUE)\nmedian_forecast &lt;- median(NFLX3$VALUE, na.rm = TRUE)\nforecast_summary &lt;- data.frame(\n  Metric = c(\"Mean forecast for 2020\", \"Median forecast for 2020\"),\n  Value = c(round(mean_forecast, 2), round(median_forecast, 2))\n)\n\nforecast_summary %&gt;%\n  kable(col.names = c(\"Metric\", \"Value\"), align = c(\"l\", \"c\"), caption = \"Forecast Summary for 2020\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE, position = \"center\")\n\n\n\nForecast Summary for 2020\n\n\nMetric\nValue\n\n\n\n\nMean forecast for 2020\n1.24\n\n\nMedian forecast for 2020\n0.41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 10: Averages\n\n\n\n\nGenerate a new dataset named NFLX4 by aggregating data from NFLX3 Include the variables size, experience, horizon, accuracy, past_accuracy, and ACTUAL in NFLX4. When calculating the yearly averages for these variables, ignore any missing values (NAs). Present a summary of the NFLX4 dataset.\nNeed Written Response in this callout: Subsequently, employ correlation analysis or exploratory data analysis to get insights into the relationships between these variables and ‘ACTUAL,’ if such relationships exist.\n\nThrough in-depth correlation analysis and exploratory data analysis of the NFLX4 data set, significant insights into the relationship between various variables and ‘ACTUAL’ earnings per share have surfaced. The data uncovers positive correlations between ‘ACTUAL’ and ‘size’ (0.18) as well as ‘experience’ (0.69), suggesting that larger analyst groups and more seasoned analysts tend to provide more accurate forecasts. In contrast, negative correlations with ‘horizon’ (-0.63) and ‘past_accuracy’ (-0.80) indicate that analysts with longer forecasting horizons and higher past accuracy might yield less precise predictions for ‘ACTUAL’ earnings per share. Detailed scatter plots further underscore these trends, including the one highlighting enhanced accuracy with an increasing number of analysts. These findings present invaluable insights for analysts, investors, and financial decision-makers, offering essential perspectives into the factors influencing earnings per share forecasts.\n\n\n\n\nCode\nNFLX4 &lt;- NFLX3 %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    size = mean(size, na.rm = TRUE),\n    experience = mean(experience, na.rm = TRUE),\n    horizon = mean(horizon, na.rm = TRUE),\n    accuracy = mean(accuracy, na.rm = TRUE),\n    past_accuracy = mean(past_accuracy, na.rm = TRUE),\n    ACTUAL = mean(ACTUAL, na.rm = TRUE)\n  )\n\nsummary(NFLX4)\n\n\n      YEAR               size         experience       horizon       \n Min.   :20021231   Min.   :1.000   Min.   :1.000   Min.   :0.06284  \n 1st Qu.:20066231   1st Qu.:1.074   1st Qu.:2.664   1st Qu.:0.08547  \n Median :20111231   Median :1.105   Median :3.400   Median :0.09289  \n Mean   :20111231   Mean   :1.132   Mean   :3.611   Mean   :0.09004  \n 3rd Qu.:20156231   3rd Qu.:1.202   3rd Qu.:4.869   3rd Qu.:0.09512  \n Max.   :20201231   Max.   :1.300   Max.   :6.061   Max.   :0.10656  \n                                                                     \n    accuracy         past_accuracy           ACTUAL       \n Min.   :-0.822085   Min.   :-0.798219   Min.   :-0.0050  \n 1st Qu.:-0.019087   1st Qu.:-0.028736   1st Qu.: 0.0914  \n Median :-0.015035   Median :-0.013423   Median : 0.2643  \n Mean   :-0.048310   Mean   :-0.060652   Mean   : 0.9248  \n 3rd Qu.:-0.005415   3rd Qu.:-0.009260   3rd Qu.: 0.5678  \n Max.   : 0.121449   Max.   :-0.001547   Max.   : 6.0800  \n                     NA's   :1                            \n\n\nCode\ncorrelation_matrix &lt;- cor(NFLX4[, c(\"size\", \"experience\", \"horizon\", \"accuracy\", \"past_accuracy\", \"ACTUAL\")], use = \"complete.obs\")\n\nprint(correlation_matrix)\n\n\n                     size  experience    horizon    accuracy past_accuracy\nsize           1.00000000  0.07451284 -0.1317823 -0.04537307    -0.1810330\nexperience     0.07451284  1.00000000 -0.4844637 -0.25882136    -0.4620906\nhorizon       -0.13178225 -0.48446371  1.0000000  0.22264895     0.4979377\naccuracy      -0.04537307 -0.25882136  0.2226489  1.00000000    -0.1604379\npast_accuracy -0.18103301 -0.46209061  0.4979377 -0.16043792     1.0000000\nACTUAL         0.18223220  0.68707354 -0.6346966 -0.31928984    -0.7958850\n                  ACTUAL\nsize           0.1822322\nexperience     0.6870735\nhorizon       -0.6346966\naccuracy      -0.3192898\npast_accuracy -0.7958850\nACTUAL         1.0000000\n\n\nCode\ncustom_colors &lt;- c(\"#4575b4\", \"#91bfdb\", \"#e0f3f8\", \"#fee090\", \"#fc8d59\", \"#d73027\")\ncorrplot(\n  correlation_matrix,\n  method = \"color\",\n  col = custom_colors,\n  type = \"upper\",\n  tl.cex = 0.7,\n  cl.cex = 0.8,\n  diag = FALSE,\n  number.cex = 0.8\n)\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = YEAR, y = experience, fill = YEAR)) +\n  geom_boxplot() +\n  ggtitle(\"Experience Distribution Across Years\") +\n  xlab(\"Year\") +\n  ylab(\"Experience\") +\n  scale_fill_manual(values = custom_colors) +  # Apply custom colors\n  theme_minimal()\n\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = accuracy, fill = YEAR)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Density Plot of Accuracy\") +\n  xlab(\"Accuracy\") +\n  ylab(\"Density\") +\n  scale_fill_manual(values = custom_colors) +  # Apply custom colors\n  theme_minimal()\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = YEAR, y = size, fill = YEAR)) +\n  geom_boxplot() +\n  ggtitle(\"Size Distribution Across Years\") +\n  xlab(\"Year\") +\n  ylab(\"Size\") +\n  scale_fill_manual(values = custom_colors) +  # Apply custom colors\n  theme_minimal()\n\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\nThe following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = horizon, fill = YEAR)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"Density Plot of Horizon\") +\n  xlab(\"Horizon\") +\n  ylab(\"Density\") +\n  scale_fill_manual(values = custom_colors) +  # Apply custom colors\n  theme_minimal()\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = ACTUAL, fill = YEAR)) +\n  geom_histogram(alpha = 0.5, bins = 30) +\n  ggtitle(\"Histogram of ACTUAL\") +\n  xlab(\"ACTUAL\") +\n  ylab(\"Frequency\") +\n  scale_fill_manual(values = custom_colors) +  # Apply custom colors\n  theme_minimal()\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nCode\ncorrelation_data &lt;- data.table::melt(correlation_matrix)\n\n\nWarning in data.table::melt(correlation_matrix): The melt generic in data.table\nhas been passed a matrix and will attempt to redirect to the relevant reshape2\nmethod; please note that reshape2 is deprecated, and this redirection is now\ndeprecated as well. To continue using melt methods from reshape2 while both\nlibraries are attached, e.g. melt.list, you can prepend the namespace like\nreshape2::melt(correlation_matrix). In the next version, this warning will\nbecome an error.\n\n\nCode\nggplot(correlation_data, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradientn(colors = custom_colors) +\n  ggtitle(\"Correlation Heatmap\") +\n  xlab(\"Variables\") +\n  ylab(\"Variables\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCode\nggplot(NFLX4, aes(x = past_accuracy, y = ACTUAL)) +\n  geom_point(color = \"#41b6c4\", shape = 20, size = 3) +\n  ggtitle(\"ACTUAL vs. past_accuracy\") +\n  xlab(\"past_accuracy\") +\n  ylab(\"ACTUAL\")\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/post with code/index.html",
    "href": "posts/post with code/index.html",
    "title": "Data framing with R",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.2\n\n\nLoading required package: lattice\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.2\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(rpart)\n\nWarning: package 'rpart' was built under R version 4.3.2\n\nlibrary(e1071) \n\nWarning: package 'e1071' was built under R version 4.3.2\n\nlibrary(kknn) \n\nWarning: package 'kknn' was built under R version 4.3.2\n\n\n\nAttaching package: 'kknn'\n\n\nThe following object is masked from 'package:caret':\n\n    contr.dummy\n\n# Function to calculate RMSE\ncalculate_rmse &lt;- function(predictions, actuals) {\n  sqrt(mean((predictions - actuals)^2))\n}\n\n# Generate random data\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\n\n# Combine data into a data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Support Vector Machine (SVM)\nsvm_model &lt;- svm(y ~ x, data = data)\n\n# k-Nearest Neighbors (k-NN)\nknn_model &lt;- kknn(y ~ x, train = data, test = data, k = 3)\n\n# Display the first few rows of the dataset\nhead(data)\n\n            x          y\n1 -0.56047565 -1.8313579\n2 -0.23017749 -0.2034713\n3  1.55870831  2.8707247\n4  0.07050839 -0.2065258\n5  0.12928774 -0.6930431\n6  1.71506499  3.3851022\n\n# Summary statistics\nsummary(data)\n\n       x                  y           \n Min.   :-2.30917   Min.   :-4.57394  \n 1st Qu.:-0.49385   1st Qu.:-1.24394  \n Median : 0.06176   Median : 0.20613  \n Mean   : 0.09041   Mean   : 0.07326  \n 3rd Qu.: 0.69182   3rd Qu.: 1.35295  \n Max.   : 2.18733   Max.   : 4.97537  \n\n# Scatter plot of X and Y\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Scatter Plot of X vs Y\",\n       x = \"X\",\n       y = \"Y\") +\n  theme_minimal()\n\n\n\n# Linear Regression\nlm_model &lt;- lm(y ~ x, data = data)\n\n# Decision Tree\ndt_model &lt;- rpart(y ~ x, data = data)\n\n# Random Forest\nrf_model &lt;- randomForest(y ~ x, data = data)\n\n# Model Evaluation\n# Make predictions\nlm_predictions &lt;- predict(lm_model, newdata = data)\ndt_predictions &lt;- predict(dt_model, newdata = data)\nrf_predictions &lt;- predict(rf_model, newdata = data)\n\n# Evaluate models\nlm_rmse &lt;- calculate_rmse(lm_predictions, data$y)\ndt_rmse &lt;- calculate_rmse(dt_predictions, data$y)\nrf_rmse &lt;- calculate_rmse(rf_predictions, data$y)\n\n# Print model evaluation results\ncat(\"Linear Regression Model:\\n\")\n\nLinear Regression Model:\n\ncat(\"RMSE:\", lm_rmse, \"\\n\\n\")\n\nRMSE: 0.9609585 \n\ncat(\"Decision Tree Model:\\n\")\n\nDecision Tree Model:\n\ncat(\"RMSE:\", dt_rmse, \"\\n\\n\")\n\nRMSE: 0.9071943 \n\ncat(\"Random Forest Model:\\n\")\n\nRandom Forest Model:\n\ncat(\"RMSE:\", rf_rmse, \"\\n\\n\")\n\nRMSE: 0.5559451 \n\n# Plot the data and regression lines for all models\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = \"green\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = \"purple\", size = 1) +\n  labs(title = \"Linear Regression, Decision Tree, and Random Forest Models\",\n       x = \"X\",\n       y = \"Y\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n# Support Vector Machine (SVM)\nsvm_model &lt;- svm(y ~ x, data = data)\n\n# k-Nearest Neighbors (k-NN)\nknn_model &lt;- kknn(y ~ x, train = data, test = data, k = 3)\n\n# Make predictions for SVM and k-NN\nsvm_predictions &lt;- predict(svm_model, newdata = data)\n# k-Nearest Neighbors (k-NN)\nknn_model &lt;- kknn(y ~ x, train = data, test = data, k = 3)\n\n# Extract predicted values from the k-NN model\nknn_predictions &lt;- as.vector(knn_model$fitted.values)\n\n# Evaluate k-NN model\nknn_rmse &lt;- calculate_rmse(knn_predictions, data$y)\n\n# Print k-NN model evaluation results\ncat(\"k-Nearest Neighbors (k-NN) Model:\\n\")\n\nk-Nearest Neighbors (k-NN) Model:\n\ncat(\"RMSE:\", knn_rmse, \"\\n\\n\")\n\nRMSE: 0.6436795 \n\n# Evaluate SVM and k-NN models\nsvm_rmse &lt;- calculate_rmse(svm_predictions, data$y)\nknn_rmse &lt;- calculate_rmse(knn_predictions, data$y)\n\n# Print additional model evaluation results\ncat(\"Support Vector Machine (SVM) Model:\\n\")\n\nSupport Vector Machine (SVM) Model:\n\ncat(\"RMSE:\", svm_rmse, \"\\n\\n\")\n\nRMSE: 0.9668863 \n\ncat(\"k-Nearest Neighbors (k-NN) Model:\\n\")\n\nk-Nearest Neighbors (k-NN) Model:\n\ncat(\"RMSE:\", knn_rmse, \"\\n\\n\")\n\nRMSE: 0.6436795 \n\n# Plot the data and regression lines for all models\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = \"green\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = \"purple\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = svm_predictions[order(data$x)]), color = \"orange\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = knn_predictions[order(data$x)]), color = \"brown\", size = 1) +\n  labs(title = \"Multiple Regression Models Comparison\",\n       x = \"X\",\n       y = \"Y\") +\n  theme_minimal()\n\n\n\n\nThe R code generates a synthetic dataset with a linear relationship between variables ‘X’ and ‘Y.’ It implements and evaluates regression models, including Linear Regression, Decision Tree, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), showcasing their predictive performance through Root Mean Squared Error (RMSE) metrics. The code concludes with a visualization comparing model predictions against the original data.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(rpart)\nlibrary(e1071) \nlibrary(kknn) \n\n# Generate random data\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\n\n# Combine data into a data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Exploratory Data Analysis (EDA)\n# (Remaining EDA code remains the same)\n\n# Support Vector Machine (SVM)\nsvm_model &lt;- svm(y ~ x, data = data)\n\n# k-Nearest Neighbors (k-NN)\nknn_model &lt;- kknn(y ~ x, train = data, test = data, k = 3)\n\n# Exploratory Data Analysis (EDA)\n\n# Display the first few rows of the dataset\nhead(data)\n\n            x          y\n1 -0.56047565 -1.8313579\n2 -0.23017749 -0.2034713\n3  1.55870831  2.8707247\n4  0.07050839 -0.2065258\n5  0.12928774 -0.6930431\n6  1.71506499  3.3851022\n\n# Summary statistics\nsummary(data)\n\n       x                  y           \n Min.   :-2.30917   Min.   :-4.57394  \n 1st Qu.:-0.49385   1st Qu.:-1.24394  \n Median : 0.06176   Median : 0.20613  \n Mean   : 0.09041   Mean   : 0.07326  \n 3rd Qu.: 0.69182   3rd Qu.: 1.35295  \n Max.   : 2.18733   Max.   : 4.97537  \n\n# Scatter plot of X and Y\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Scatter Plot of X vs Y\",\n       x = \"X\",\n       y = \"Y\") +\n  theme_minimal()\n\n\n\n# Linear Regression\nlm_model &lt;- lm(y ~ x, data = data)\n\n# Decision Tree\ndt_model &lt;- rpart(y ~ x, data = data)\n\n# Random Forest\nrf_model &lt;- randomForest(y ~ x, data = data)\n\n# Model Evaluation\n# Function to calculate RMSE\ncalculate_rmse &lt;- function(predictions, actuals) {\n  sqrt(mean((predictions - actuals)^2))\n}\n\n# Make predictions\nlm_predictions &lt;- predict(lm_model, newdata = data)\ndt_predictions &lt;- predict(dt_model, newdata = data)\nrf_predictions &lt;- predict(rf_model, newdata = data)\n\n# Evaluate models\nlm_rmse &lt;- calculate_rmse(lm_predictions, data$y)\ndt_rmse &lt;- calculate_rmse(dt_predictions, data$y)\nrf_rmse &lt;- calculate_rmse(rf_predictions, data$y)\n\n# Print model evaluation results\ncat(\"Linear Regression Model:\\n\")\n\nLinear Regression Model:\n\ncat(\"RMSE:\", lm_rmse, \"\\n\\n\")\n\nRMSE: 0.9609585 \n\ncat(\"Decision Tree Model:\\n\")\n\nDecision Tree Model:\n\ncat(\"RMSE:\", dt_rmse, \"\\n\\n\")\n\nRMSE: 0.9071943 \n\ncat(\"Random Forest Model:\\n\")\n\nRandom Forest Model:\n\ncat(\"RMSE:\", rf_rmse, \"\\n\\n\")\n\nRMSE: 0.5559451 \n\n# Plot the data and regression lines for all models\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = \"green\", size = 1) +\n  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = \"purple\", size = 1) +\n  labs(title = \"Linear Regression, Decision Tree, and Random Forest Models\",\n       x = \"X\",\n       y = \"Y\") +\n  theme_minimal()\n\n\n\n# Support Vector Machine (SVM)\nsvm_model &lt;- svm(y ~ x, data = data)\n\nThe R code performs exploratory data analysis (EDA) on a synthetic dataset and evaluates multiple regression models, including Linear Regression, Decision Tree, and Random Forest. It generates a scatter plot, calculates summary statistics, and assesses model performance using Root Mean Squared Error (RMSE), presenting a visual comparison of model predictions against the original data."
  }
]