lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Model Evaluation
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)  # Adding the e1071 package for SVM
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# (Remaining EDA code remains the same)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Model Evaluation
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- knn.reg(train = data[, "x", drop = FALSE], test = data[, "y", drop = FALSE], k = 3)
install.packages("kknn")
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# (Remaining EDA code remains the same)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, data = data, k = 3)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# (Remaining EDA code remains the same)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, data = data, k = 3)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# (Remaining EDA code remains the same)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Exploratory Data Analysis (EDA)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Model Evaluation
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- knn.reg(train = data[, "x", drop = FALSE], test = data[, "y", drop = FALSE], k = 3)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Exploratory Data Analysis (EDA)
# (Remaining EDA code remains the same)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Exploratory Data Analysis (EDA)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Model Evaluation
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Model Evaluation
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Make predictions for SVM and k-NN
svm_predictions <- predict(svm_model, newdata = data)
knn_predictions <- as.vector(predict(knn_model, data))
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(kknn)
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
sqrt(mean((predictions - actuals)^2))
}
# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
# Combine data into a data frame
data <- data.frame(x = x, y = y)
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Display the first few rows of the dataset
head(data)
# Summary statistics
summary(data)
# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
labs(title = "Scatter Plot of X vs Y",
x = "X",
y = "Y") +
theme_minimal()
# Linear Regression
lm_model <- lm(y ~ x, data = data)
# Decision Tree
dt_model <- rpart(y ~ x, data = data)
# Random Forest
rf_model <- randomForest(y ~ x, data = data)
# Model Evaluation
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)
# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)
# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")
cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")
cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
x = "X",
y = "Y") +
theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Make predictions for SVM and k-NN
svm_predictions <- predict(svm_model, newdata = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)
# Extract predicted values from the k-NN model
knn_predictions <- as.vector(knn_model$fitted.values)
# Evaluate k-NN model
knn_rmse <- calculate_rmse(knn_predictions, data$y)
# Print k-NN model evaluation results
cat("k-Nearest Neighbors (k-NN) Model:\n")
cat("RMSE:", knn_rmse, "\n\n")
# Evaluate SVM and k-NN models
svm_rmse <- calculate_rmse(svm_predictions, data$y)
knn_rmse <- calculate_rmse(knn_predictions, data$y)
# Print additional model evaluation results
cat("Support Vector Machine (SVM) Model:\n")
cat("RMSE:", svm_rmse, "\n\n")
cat("k-Nearest Neighbors (k-NN) Model:\n")
cat("RMSE:", knn_rmse, "\n\n")
# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
geom_point(color = "blue") +
geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = svm_predictions[order(data$x)]), color = "orange", size = 1) +
geom_line(data = data.frame(x = sort(data$x), y = knn_predictions[order(data$x)]), color = "brown", size = 1) +
labs(title = "Multiple Regression Models Comparison",
x = "X",
y = "Y") +
theme_minimal()
