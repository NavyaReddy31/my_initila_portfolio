---
title: "Data framing with R"
author: "Navya reddy"
date: "2023-12-08"
categories: [news, code, analysis,plotly,plot]
image: "b.jpg"
---

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071) 
library(kknn) 

# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
  sqrt(mean((predictions - actuals)^2))
}

# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Combine data into a data frame
data <- data.frame(x = x, y = y)

# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)

# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)

# Display the first few rows of the dataset
head(data)

# Summary statistics
summary(data)

# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot of X vs Y",
       x = "X",
       y = "Y") +
  theme_minimal()

# Linear Regression
lm_model <- lm(y ~ x, data = data)

# Decision Tree
dt_model <- rpart(y ~ x, data = data)

# Random Forest
rf_model <- randomForest(y ~ x, data = data)

# Model Evaluation
# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)

# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)

# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")

cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")

cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")

# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
  labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
       x = "X",
       y = "Y") +
  theme_minimal()

# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)

# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)

# Make predictions for SVM and k-NN
svm_predictions <- predict(svm_model, newdata = data)
# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)

# Extract predicted values from the k-NN model
knn_predictions <- as.vector(knn_model$fitted.values)

# Evaluate k-NN model
knn_rmse <- calculate_rmse(knn_predictions, data$y)

# Print k-NN model evaluation results
cat("k-Nearest Neighbors (k-NN) Model:\n")
cat("RMSE:", knn_rmse, "\n\n")


# Evaluate SVM and k-NN models
svm_rmse <- calculate_rmse(svm_predictions, data$y)
knn_rmse <- calculate_rmse(knn_predictions, data$y)

# Print additional model evaluation results
cat("Support Vector Machine (SVM) Model:\n")
cat("RMSE:", svm_rmse, "\n\n")

cat("k-Nearest Neighbors (k-NN) Model:\n")
cat("RMSE:", knn_rmse, "\n\n")

# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = svm_predictions[order(data$x)]), color = "orange", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = knn_predictions[order(data$x)]), color = "brown", size = 1) +
  labs(title = "Multiple Regression Models Comparison",
       x = "X",
       y = "Y") +
  theme_minimal()
```

***The R code generates a synthetic dataset with a linear relationship between variables 'X' and 'Y.' It implements and evaluates regression models, including Linear Regression, Decision Tree, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), showcasing their predictive performance through Root Mean Squared Error (RMSE) metrics. The code concludes with a visualization comparing model predictions against the original data.***

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(e1071) 
library(kknn) 

# Generate random data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Combine data into a data frame
data <- data.frame(x = x, y = y)

# Exploratory Data Analysis (EDA)
# (Remaining EDA code remains the same)

# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)

# k-Nearest Neighbors (k-NN)
knn_model <- kknn(y ~ x, train = data, test = data, k = 3)

# Exploratory Data Analysis (EDA)

# Display the first few rows of the dataset
head(data)

# Summary statistics
summary(data)

# Scatter plot of X and Y
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot of X vs Y",
       x = "X",
       y = "Y") +
  theme_minimal()

# Linear Regression
lm_model <- lm(y ~ x, data = data)

# Decision Tree
dt_model <- rpart(y ~ x, data = data)

# Random Forest
rf_model <- randomForest(y ~ x, data = data)

# Model Evaluation
# Function to calculate RMSE
calculate_rmse <- function(predictions, actuals) {
  sqrt(mean((predictions - actuals)^2))
}

# Make predictions
lm_predictions <- predict(lm_model, newdata = data)
dt_predictions <- predict(dt_model, newdata = data)
rf_predictions <- predict(rf_model, newdata = data)

# Evaluate models
lm_rmse <- calculate_rmse(lm_predictions, data$y)
dt_rmse <- calculate_rmse(dt_predictions, data$y)
rf_rmse <- calculate_rmse(rf_predictions, data$y)

# Print model evaluation results
cat("Linear Regression Model:\n")
cat("RMSE:", lm_rmse, "\n\n")

cat("Decision Tree Model:\n")
cat("RMSE:", dt_rmse, "\n\n")

cat("Random Forest Model:\n")
cat("RMSE:", rf_rmse, "\n\n")

# Plot the data and regression lines for all models
ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue") +
  geom_abline(intercept = coef(lm_model)[1], slope = coef(lm_model)[2], color = "red", linetype = "dashed", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(dt_model, newdata = data.frame(x = sort(data$x)))), color = "green", size = 1) +
  geom_line(data = data.frame(x = sort(data$x), y = predict(rf_model, newdata = data.frame(x = sort(data$x)))), color = "purple", size = 1) +
  labs(title = "Linear Regression, Decision Tree, and Random Forest Models",
       x = "X",
       y = "Y") +
  theme_minimal()
# Support Vector Machine (SVM)
svm_model <- svm(y ~ x, data = data)

```

***`The R code performs exploratory data analysis (EDA) on a synthetic dataset and evaluates multiple regression models, including Linear Regression, Decision Tree, and Random Forest. It generates a scatter plot, calculates summary statistics, and assesses model performance using Root Mean Squared Error (RMSE), presenting a visual comparison of model predictions against the original data.`***
